{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfbd1fa2-67e1-4089-b1a5-7f8c62b78cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# CCI Literature Analysis\n",
    "\n",
    "This notebook queries PubMed Central to extract full-text XML files of articles on a pre-defined list. After preprocessing the text and extracting paragraphs that reference one of ten Charlson Comorbidity Index versions we are interested in, each paragraph is analyzed with a Large Language Model using the Azure AI Foundry endpoint to extract the relevant references. \n",
    "\n",
    "Author: Josh Fuchs\n",
    "\n",
    "Copyright 2025, The University of North Carolina at Chapel Hill. Permission is granted to use in accordance with the MIT license. The code is licensed under the open-source MIT license.\n",
    "\n",
    "This software uses the Entrez Programming Utilities available from the National Center for Biotechnology Information (NCBI). Please carefully review the NCBI's Disclaimer and Copyright notice at https://www.ncbi.nlm.nih.gov/home/about/policies/ before use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e93f63-3640-4149-92e6-c112331c1436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from lxml import etree\n",
    "\n",
    "import urllib \n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import configparser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "import xml.dom.minidom\n",
    "from bs4 import BeautifulSoup, element\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37fd9e4d-fa45-4979-9112-d96de9ac0675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999f15bf-e3ee-49c0-8cc7-e352addc6ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Endpoint and API keys from config.ini\n",
    "\n",
    "# Create a ConfigParser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "\n",
    "# Access values from the configuration file\n",
    "endpoint_url = config.get('azure_foundry', 'endpoint_url')\n",
    "azure_key = config.get('azure_foundry', 'key')\n",
    "\n",
    "entrez_key = config.get('entrez','entrez_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd83903-ceae-4c1c-9f71-f3b1a1d1e362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the search criteria: \n",
    "# each element of the list is a different version of CCI\n",
    "# we are searching for.\n",
    "search_criteria = [\n",
    "    {\n",
    "        'title': 'Updating and validating the Charlson Comorbidity Index and Score for risk adjustment in hospital discharge abstracts using data from 6 countries',\n",
    "        'year': '2011',\n",
    "        'surname': 'Quan'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Coding algorithms for defining comorbidities in ICD-9-CM and ICD-10 administrative data',\n",
    "        'year': '2005',\n",
    "        'surname': 'Quan'\n",
    "    },\n",
    "    {\n",
    "        'title': 'New ICD-10 version of the Charlson comorbidity index predicted in-hospital mortality',\n",
    "        'year': '2004',\n",
    "        'surname': 'Sundararajan'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Improved Comorbidity Adjustment for Predicting Mortality in Medicare Populations',\n",
    "        'year': '2003',\n",
    "        'surname': 'Schneeweiss'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Measuring potentially avoidable hospital readmissions',\n",
    "        'year': '2002',\n",
    "        'surname': 'Halfon'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Development of a comorbidity index using physician claims data',\n",
    "        'year': '2000',\n",
    "        'surname': 'Klabunde'\n",
    "    },\n",
    "        {\n",
    "        'title': 'Validation of a combined comorbidity index',\n",
    "        'year': '1994',\n",
    "        'surname': 'Charlson'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Adapting a clinical comorbidity index for use with ICD-9-CM administrative data: differing perspectives',\n",
    "        'year': '1993',\n",
    "        'surname': 'Romano'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Adapting a clinical comorbidity index for use with ICD-9-CM administrative databases',\n",
    "        'year': '1992',\n",
    "        'surname': 'Deyo'\n",
    "    },\n",
    "    {\n",
    "        'title': 'A new method of classifying prognostic comorbidity in longitudinal studies: development and validation',\n",
    "        'year': '1987',\n",
    "        'surname': 'Charlson'\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fbe705-7334-42e1-8117-1b4b7360f800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_references_for_search(criteria: list[dict]) -> list[str]:\n",
    "    '''\n",
    "    Formats each CCI version paper as one of the following. This accounts for varieties\n",
    "    in how references are formatted in the body of the text, so that we can automate the text searching\n",
    "    while accounting for varieties in how references appear:\n",
    "    Charlson 1987\n",
    "    Charlson et al. 1987\n",
    "    Charlson et al. (1987)\n",
    "    Charlson et\\xa0al., 1987\n",
    "    Charlson et\\xa0al. 1987\n",
    "    Charlson et al., 1987\n",
    "    Charlson et\\xa0al. (1987)\n",
    "    Charlson et al.\\xa0(1987)\n",
    "    Charlson et al, 1987\n",
    "    Charlsonetal., 1987\n",
    "\n",
    "    PARAMETERS\n",
    "    criteria : list of dictionaries\n",
    "        each element of the list is a different major version of Charlson\n",
    "        each dictionary should contain the title, year, and surname of first author\n",
    "    \n",
    "    OUTPUT\n",
    "    result : list \n",
    "        formatted list of strings for each major version and different format\n",
    "\n",
    "    '''\n",
    "    result = []\n",
    "    for item in criteria:\n",
    "        result.append(f\"{item['surname']} {item['year']}\")\n",
    "        result.append(f\"{item['surname']} et al. {item['year']}\")\n",
    "        result.append(f\"{item['surname']} et al. ({item['year']})\")\n",
    "        result.append(f\"{item['surname']} et\\xa0al., {item['year']}\")\n",
    "        result.append(f\"{item['surname']} et\\xa0al. {item['year']}\")\n",
    "        result.append(f\"{item['surname']} et al., {item['year']}\")\n",
    "        result.append(f\"{item['surname']} et\\xa0al. ({item['year']})\")\n",
    "        result.append(f\"{item['surname']} et al.\\xa0({item['year']})\")\n",
    "        result.append(f\"{item['surname']} et al, {item['year']}\")\n",
    "        result.append(f\"{item['surname']}et al., {item['year']}\")\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc2e659-8fd2-4848-aaba-05f17550b639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate the list of CCI version references we will search\n",
    "# for. We will search each paragraph for these terms to keep for analysis. \n",
    "# Explicitly searching for the references also helps us to identify and analyze papers\n",
    "# that reference a CCI version, but do not include it in the list of references. \n",
    "# See PMC8505350 for an example of this, which is rare.  \n",
    "\n",
    "# Format variations for references\n",
    "charlson_ref_versions = format_references_for_search(search_criteria)\n",
    "\n",
    "# Other very unusual reference formatting\n",
    "# From PMCID: PMC7063690\n",
    "charlson_unusual = ['Charlson, Pompei, Ales, & MacKenzie, 1987']\n",
    "\n",
    "charlson_search_terms = charlson_ref_versions + charlson_unusual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db22c8d8-845c-4430-86a8-22a1788a3fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract article as XML From PMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861cf0dc-9636-469d-a4dd-45f9aefceaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetchPMCxml(pmc_id: str,entrez_key:str) -> BeautifulSoup:\n",
    "    '''\n",
    "    Uses the Entrez module in BioPython to access the PMC database. Articles are returned as xml files, then\n",
    "    converted into BeautifulSoup objects. If the query to PMC does not return an article, the function returns None.\n",
    "\n",
    "    Email is hard-coded in. Email is required from the NCBI. \n",
    "    API key allows up to 10 queries/sec, as opposed to 3 without.\n",
    "\n",
    "    See https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/ for details about\n",
    "    getting an API key for the NCBI. \n",
    "\n",
    "    PARAMETERS\n",
    "    pmc_id : string\n",
    "        string that is the PMC ID formatted like \"9651183\"\n",
    "    \n",
    "    entrez_key : string\n",
    "        API key for NCBI to allow up to 10 queries/sec\n",
    "\n",
    "    RETURNS\n",
    "    soup : BeautifulSoup object\n",
    "        object that contains the article\n",
    "    \n",
    "    Returns None if query to PMC fails.    \n",
    "    '''\n",
    "    Entrez.email = \"YOUR_EMAIL@MAIL.EDU\" # you must give NCBI an email address\n",
    "    Entrez.api_key = entrez_key \n",
    "    \n",
    "    try:\n",
    "        fetchHandle = Entrez.efetch(db=\"pmc\", retmax=1, retmode=\"xml\", id=pmc_id)\n",
    "        data = fetchHandle.read()\n",
    "        fetchHandle.close()\n",
    "        soup = BeautifulSoup(data,\"xml\")\n",
    "        return soup\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74bd9249-c451-4e69-8635-0c6e42474698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Find CCI References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec66a02-d1d1-4d86-bf8a-9c1589323509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compare_strings(article_title: str, criteria_title: str) -> bool:\n",
    "    \"\"\"\n",
    "    Compares two strings and returns True if at least 80% of the words in article_title are in criteria_title.\n",
    "    This helps confirm that the titles we are looking for are actually the titles, while accounting for\n",
    "    typos in the title, so we don't have to require an exact match.\n",
    "\n",
    "    PARAMETERS\n",
    "    article_title : string\n",
    "        title of the reference in the paper being analyzed\n",
    "\n",
    "    criteria_title : string\n",
    "        title of the Charlson version paper being compared to\n",
    "\n",
    "    RETURNS\n",
    "        boolean\n",
    "        True if at least 80% of the words match\n",
    "        Otherwise False\n",
    "    \"\"\"\n",
    "    # Split the titles into words\n",
    "    article_words_punctuation = article_title.lower().split()\n",
    "    criteria_words_punctuation = criteria_title.lower().split()\n",
    "\n",
    "    # Remove leading and trailing punctuation from each word\n",
    "    article_words = [word.strip(string.punctuation) for word in article_words_punctuation]\n",
    "    criteria_words = [word.strip(string.punctuation) for word in criteria_words_punctuation]\n",
    "\n",
    "    # Count the number of words in article_title that are in criteria_title\n",
    "    matching_words = sum(1 for word in article_words if word in criteria_words)\n",
    "    \n",
    "    # Calculate the percentage of matching words\n",
    "    # If there are no words in article_title, return 0.0\n",
    "    if len(article_words) == 0:\n",
    "        match_percentage = 0.0\n",
    "    else:\n",
    "        match_percentage = matching_words / len(article_words)\n",
    "    \n",
    "    # Return True if at least 80% of the words match, otherwise False\n",
    "    return match_percentage >= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e21ac0-6bc0-4d91-ba0f-2ed8017e2c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_charlson_references(soup: BeautifulSoup,search_criteria: list[dict]) -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Parses an XML document represented by a BeautifulSoup object to find references\n",
    "    and matches them against a list of search criteria. The matching references are returned as a dictionary\n",
    "    with reference labels as keys and formatted reference strings as values.\n",
    "\n",
    "    PARAMETERS\n",
    "    soup : BeautifulSoup object\n",
    "        A BeautifulSoup object representing the parsed XML document.\n",
    "    \n",
    "    search_criteria : list of dict\n",
    "        A list of dictionaries containing search criteria. Each dictionary should have the keys:\n",
    "            - 'title' (str): The title of the reference to match.\n",
    "            - 'year' (str): The year of the reference to match.\n",
    "            - 'surname' (str): The surname to include in the formatted reference string.\n",
    "\n",
    "    RETRUNS\n",
    "    charlson_references : dict\n",
    "        A dictionary where the keys are reference labels (int) and the values are formatted reference strings (str).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty dictionary to store the charlson_references\n",
    "    charlson_references = {}\n",
    "\n",
    "    # Find all references\n",
    "    refs = soup.find_all('ref')\n",
    "\n",
    "    # Iterate through each reference\n",
    "    for ref in refs:\n",
    "        # Try to find the label tag first\n",
    "        label_tag = ref.find('label')\n",
    "\n",
    "        if label_tag:\n",
    "            label = label_tag.text\n",
    "            # Remove non-digits from the label\n",
    "            if isinstance(label, str):\n",
    "                label = ''.join(filter(str.isdigit, label))\n",
    "        else:\n",
    "            # If label tag is not found, use the id attribute\n",
    "            # to format the label based on how the id is formatted\n",
    "            label = ref.get('id')\n",
    "            if label and 'bib-' in label:\n",
    "                label = label.split('bib-', 1)[1]\n",
    "            elif label and label.startswith('bibr'):\n",
    "                # for references formatted as bibr1-23337214241284181\n",
    "                label = re.search(r'bibr(\\d+)-', label).group(1)\n",
    "            elif label and label.startswith(('bib','BIB','ref','REF','CIT','cit')):\n",
    "                label = label[3:]\n",
    "            elif label and label.startswith(('bb','CR')):\n",
    "                label = label[2:]\n",
    "            elif label and label.startswith(('B','C','b','c','R','r')):\n",
    "                label = label[1:]\n",
    "\n",
    "        article_title_tag = ref.find('article-title')\n",
    "        ref_year_tag = ref.find('year')\n",
    "        \n",
    "        # Check if the article_title_tag and ref_year_tag are not None\n",
    "        if article_title_tag and ref_year_tag:\n",
    "            article_title = article_title_tag.text\n",
    "            ref_year = ref_year_tag.text\n",
    "            \n",
    "            # Check if the title and year match any of the given criteria\n",
    "            for criteria in search_criteria:\n",
    "                # we'll implement a criteria that at least 80% of the words in the title need to match\n",
    "                # this will account for instances of titles missing a word or two\n",
    "                if compare_strings(article_title, criteria['title']) and ref_year == criteria['year']:\n",
    "                    charlson_references[int(label)] = f\"{criteria['surname']} {criteria['year']}\"\n",
    "        else:\n",
    "            # Some references combine authors and title into a single tag called mixed-citation publication-type\n",
    "            mixed_citation_tag = ref.find('mixed-citation')\n",
    "            if mixed_citation_tag:\n",
    "                # Extract the year and title from the mixed-citation tag\n",
    "                mixed_citation_text = mixed_citation_tag.text\n",
    "                xml_year = mixed_citation_tag.find('year').text if mixed_citation_tag.find('year') else None\n",
    "                xml_title = mixed_citation_text#.split('.')[1].strip() if '.' in mixed_citation_text else None\n",
    "\n",
    "                # Check if the year and title match any of the given criteria\n",
    "                for criteria in search_criteria:\n",
    "                    if xml_title and xml_year and criteria['title'] in xml_title and criteria['year'] == xml_year:\n",
    "                        charlson_references[int(label)] = f\"{criteria['surname']} {criteria['year']}\"\n",
    "\n",
    "    # Return the dictionary of matching charlson_references\n",
    "    return charlson_references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc444f9f-9bb5-4b07-a453-3bd449164df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Search text for references and replace\n",
    "\n",
    "Function to search the text for each CCI version reference and replaces the XML reference with the text reference. This provides reformatting to enable paragraph selection and data extraction from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f5b93d-11cf-49ae-9c88-d7210db5f667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_references_in_body(soup: BeautifulSoup, references: dict[int,str]) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    This function processes an XML document represented by a BeautifulSoup object to find and replace\n",
    "    reference tags (`xref`) and formatted references (e.g., [2]) in the body text with corresponding\n",
    "    reference strings from a given dictionary. The updated XML document is returned.\n",
    "\n",
    "    PARAMETERS\n",
    "    soup : BeautifulSoup object\n",
    "        A BeautifulSoup object representing the parsed XML document.\n",
    "    references : dict\n",
    "        A dictionary where the keys are reference numbers (int) and the values are formatted reference strings (str).\n",
    "\n",
    "    RETURNS\n",
    "    updated_soup : BeautifulSoup object\n",
    "        A BeautifulSoup object representing the updated XML document with replaced references.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the soup so you don't modify the original\n",
    "    soup_copy = deepcopy(soup)\n",
    "    # Find all xref tags in the body of the XML document\n",
    "    xrefs = soup_copy.find_all('xref')\n",
    "    updated_soup = None\n",
    "\n",
    "    # If xrefs is empty, that most likely means the references are embedded as [2] in the text. In this case, we'll search for those \n",
    "    # and replace them as appropriate.\n",
    "    if not xrefs:\n",
    "        # If xrefs is empty, look for references formatted as [5]\n",
    "        body_text = str(soup_copy)\n",
    "        formatted_refs = re.findall(r'\\[\\d+\\]', body_text)\n",
    "\n",
    "        for formatted_ref in formatted_refs:\n",
    "            ref_number = formatted_ref.strip('[]')\n",
    "            if ref_number.isdigit() and int(ref_number) in references:\n",
    "                replacement_text = f\"({references[int(ref_number)]})\"\n",
    "                body_text = body_text.replace(formatted_ref, replacement_text)\n",
    "\n",
    "        # Update the soup with the modified body text\n",
    "        updated_soup = BeautifulSoup(body_text, 'html.parser')\n",
    "    else:\n",
    "        # Iterate through each xref tag\n",
    "        for xref in xrefs:\n",
    "            ref_text = xref.text.strip()\n",
    "\n",
    "            expanded_refs = []\n",
    "\n",
    "            # Split the reference text by commas\n",
    "            parts = ref_text.split(',')\n",
    "\n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                # Check if the part contains a range\n",
    "                if '–' in part:\n",
    "                    start_end = part.split('–')\n",
    "                    if len(start_end) == 2 and start_end[0].strip().isdigit() and start_end[1].strip().isdigit():\n",
    "                        start, end = map(int, start_end)\n",
    "                        expanded_refs.extend([str(i) for i in range(start, end + 1)])\n",
    "                    else:\n",
    "                        # This is for rare cases where part = online supplementary file – database\n",
    "                        expanded_refs.append(part)\n",
    "                # Next check is reference number is stored between square brackets\n",
    "                elif '[' in part and ']' in part:\n",
    "                    ref_number_only = part.strip('[]')\n",
    "                    expanded_refs.append(ref_number_only)\n",
    "                else:\n",
    "                    expanded_refs.append(part)\n",
    "                    \n",
    "            # Replace each reference number in the expanded list\n",
    "            replacement_texts = []\n",
    "            for ref_number in expanded_refs:\n",
    "                if ref_number.isdigit() and int(ref_number) in references:\n",
    "                    replacement_texts.append(f\"({references[int(ref_number)]})\")\n",
    "                else:\n",
    "                    replacement_texts.append(ref_number)\n",
    "\n",
    "            # Replace the whole xref tag with the expanded references\n",
    "            xref.replace_with(', '.join(replacement_texts))\n",
    "        updated_soup = soup_copy\n",
    "    return updated_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2575cd42-125c-4e88-9a31-6ccd9f4b2f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Identify paragraphs with CCI version references\n",
    "\n",
    "Parse the text into paragraphs, then identify which paragraphs include CCI version references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee85091-d8b8-4490-95fc-c75996116cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These functions are slightly modified from \n",
    "# pubmed_parser to work with XML data already\n",
    "# loaded into the notebook from PMC\n",
    "\n",
    "def stringify_children(node):\n",
    "    \"\"\"Joins all string parts excluding empty parts.\"\"\"\n",
    "    return \"\".join(text.strip() for text in node.itertext() if text)\n",
    "\n",
    "def parse_article_meta(tree):\n",
    "    \"\"\"\n",
    "    Parse PMID, PMC and DOI from given article tree\n",
    "    \"\"\"\n",
    "    article_meta = tree.find(\".//article-meta\")\n",
    "    if article_meta is not None:\n",
    "        pmid_node = article_meta.find('article-id[@pub-id-type=\"pmid\"]')\n",
    "        pmc_node = article_meta.find('article-id[@pub-id-type=\"pmc\"]')\n",
    "        pub_id_node = article_meta.find('article-id[@pub-id-type=\"publisher-id\"]')\n",
    "        doi_node = article_meta.find('article-id[@pub-id-type=\"doi\"]')\n",
    "    else:\n",
    "        pmid_node = None\n",
    "        pmc_node = None\n",
    "        pub_id_node = None\n",
    "        doi_node = None\n",
    "\n",
    "    pmid = pmid_node.text if pmid_node is not None else \"\"\n",
    "    pmc = pmc_node.text if pmc_node is not None else \"\"\n",
    "    pub_id = pub_id_node.text if pub_id_node is not None else \"\"\n",
    "    doi = doi_node.text if doi_node is not None else \"\"\n",
    "\n",
    "    dict_article_meta = {\"pmid\": pmid, \"pmc\": pmc, \"doi\": doi, \"publisher_id\": pub_id}\n",
    "\n",
    "    return dict_article_meta\n",
    "\n",
    "def parse_pubmed_paragraph(input_data):\n",
    "    \"\"\"\n",
    "    Give path to a given PubMed OA file, parse and return\n",
    "    a dictionary of all paragraphs, section that it belongs to,\n",
    "    and a list of reference made in each paragraph as a list of PMIDs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data: str or bs4.element.Tag\n",
    "        A string to an XML path or a BeautifulSoup Tag object.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    dict_pars: list\n",
    "        A list contains dictionary for paragraph text and its metadata.\n",
    "        Metadata includes 'pmc' of an article, 'pmid' of an article,\n",
    "        'reference_ids' which is a list of reference ``rid`` made in a paragraph,\n",
    "        'section' name of an article, and section 'text'\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, bytes):\n",
    "        input_data = input_data.decode('utf-8')  # Decode bytes to string\n",
    "\n",
    "    if isinstance(input_data, str) and input_data.strip().startswith('<'):\n",
    "        # path is an XML string\n",
    "        tree = etree.fromstring(input_data)\n",
    "    elif isinstance(input_data, element.Tag):\n",
    "        # input_data is a BeautifulSoup Tag object\n",
    "        xml_str = str(input_data)\n",
    "        # Remove XML encoding declaration\n",
    "        xml_str = re.sub(r'<\\?xml.*?\\?>', '', xml_str)\n",
    "        tree = etree.fromstring(xml_str)\n",
    "    else:   \n",
    "        # path is a path to an XML file\n",
    "        tree = etree.parse(input_data)\n",
    "\n",
    "\n",
    "    dict_article_meta = parse_article_meta(tree)\n",
    "    pmid = dict_article_meta[\"pmid\"]\n",
    "    pmc = dict_article_meta[\"pmc\"]\n",
    "    paragraphs = tree.xpath(\"//body//p\")\n",
    "    dict_pars = list()\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_text = stringify_children(paragraph)\n",
    "        section = paragraph.find(\"../title\")\n",
    "        if section is not None:\n",
    "            section = stringify_children(section).strip()\n",
    "        else:\n",
    "            section = \"\"\n",
    "\n",
    "        ref_ids = list()\n",
    "        for xref in paragraph.xpath(\".//xref[@ref-type='bibr']\"):\n",
    "            if \"rid\" in xref.attrib:\n",
    "                ref_id = xref.attrib[\"rid\"]\n",
    "                ref_ids.append(ref_id)\n",
    "\n",
    "        dict_par = {\n",
    "            \"pmc\": pmc,\n",
    "            \"pmid\": pmid,\n",
    "            \"section\": section,\n",
    "            \"text\": paragraph_text,\n",
    "        }\n",
    "\n",
    "        dict_pars.append(dict_par)\n",
    "\n",
    "    return dict_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d2cd48-03b8-4208-a09f-561d8c3d0df7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_paragraphs(paragraph_dictionary: list[dict], charlson_terms: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Filters paragraphs based on the presence of specified terms.\n",
    "\n",
    "    This function iterates through a list of dictionaries, each representing a paragraph, and checks\n",
    "    if any of the specified terms are present in the text of the paragraph. If a match is found, that\n",
    "    paragraph is added to the list of matching paragraphs.\n",
    "\n",
    "    PARAMETERS\n",
    "    paragraph_dictionary : list of dict\n",
    "        A list of dictionaries, each containing a 'text' field representing the paragraph text.\n",
    "    charlson_terms : list of str\n",
    "        A list of terms to search for within the paragraph text.\n",
    "\n",
    "    RETURNS\n",
    "    matching_paragraphs : list of dict\n",
    "        A list of dictionaries containing paragraphs that match any of the specified terms.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store dictionaries with matching text\n",
    "    matching_paragraphs = []\n",
    "\n",
    "    # Iterate over each dictionary in the paragraph_dictionary list\n",
    "    for paragraph in paragraph_dictionary:\n",
    "        # Convert the text field to lowercase\n",
    "        paragraph_text_lower = paragraph['text'].lower()\n",
    "\n",
    "        # Check if any element of charlson_terms is part of the text field\n",
    "        if any(ref.lower() in paragraph_text_lower for ref in charlson_terms):\n",
    "            # If a match is found, add the dictionary to the matching_paragraphs list\n",
    "            matching_paragraphs.append(paragraph)\n",
    "\n",
    "    return matching_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9086e7f-439d-4936-a123-7d930d869ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define functions and variables for LLM data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060a8878-2bee-4efd-a157-9727a32db442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the model name and create a ChatCompletionsClient object\n",
    "model_name = \"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint_url,\n",
    "    credential=AzureKeyCredential(azure_key),\n",
    "    model=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb7882f-117a-442b-aee3-0e33af80cb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def query(question: dict[str,str],model_name: str) -> tuple[str, str, int]:\n",
    "    \"\"\"\n",
    "    Queries the ChatCompletions API and returns the response\n",
    "\n",
    "    Note that max_tokens has been deprecated and replaced with max_completion_tokens in\n",
    "    the OpenAI Guide. This version from Azure is behind that version. \n",
    "\n",
    "    PARAMETERS\n",
    "    question : input for messages to send to the API.\n",
    "        dictionary using the format: {'role': 'content', 'content': 'your message'}\n",
    "        or list using AssistantMessage, SystemMessage, and/or UserMessage\n",
    "    model_name : string\n",
    "        model name of the model to use, from Azure AI Foundry\n",
    "\n",
    "    RETURNS\n",
    "    question : input for messages to send to the API.\n",
    "        returns the input question to assist with tracking\n",
    "    response_content : string\n",
    "        the content response from the API\n",
    "    total_tokens : int\n",
    "        the total number of tokens used (query + response)\n",
    "    \"\"\"\n",
    "    response = client.complete(\n",
    "        messages=question,\n",
    "        max_tokens=512,\n",
    "        temperature=0,\n",
    "        top_p=0.1,\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "        model=model_name\n",
    "        )\n",
    "    \n",
    "    response_content = response.choices[0].message.content\n",
    "    total_tokens = response.usage.total_tokens\n",
    "\n",
    "    return(question,response_content,total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96b9f9c-f7e0-4605-b5ea-0b4d205d38d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Content/System Messages and the series of questions\n",
    "\n",
    "system_message0 = \"\"\"You are a literature review assistant. You will carefully review paragraphs and answer based on only what is present in the text. We are analyzing published literature to determine if it is clear which reference or references were used to calculate the Charlson Comorbidity Index (CCI) in this text. The text might include references that were not used to calculate the CCI, it might include just a single reference that was used, or it might contain multiple references that were used. \"\"\"\n",
    "\n",
    "q0 = \"\"\"You are an advanced language model tasked with analyzing a provided paragraph from a published research article. Your goal is to determine whether the text explicitly states that the Charlson Comorbidity Index (CCI) was calculated in the study. Please follow these guidelines: 1. Read the paragraph carefully and identify any mention of the Charlson Comorbidity Index. 2. Look for keywords and phrases that indicate calculation or use of the CCI, such as \"calculated\", \"assessed\", \"used\", or similar terms. 3. If the paragraph suggests that the CCI was calculated or used in the analysis, respond with \"Yes\" 4. If the paragraph does not mention calculation or use of the CCI respond with \"No\" 5. Your response should be limited to \"Yes\" or \"No\" only, without any additional commentary or explanation. Please provide your response based on the analysis of the text provided. Don't  be strict. \"\"\"\n",
    "\n",
    "q1 = \"\"\"Answer Yes or No only. Does the following text reference which paper or weights was used to calculate the Charlson Comorbidity Index (CCI)? \"\"\"\n",
    "q2 = \"\"\"Answer Yes or No only. Does the following paragraph contain more than 1 reference to how the Charlson Comorbidity Index (CCI) was calculated? \"\"\"\n",
    "\n",
    "# The following is only asked if question 2 = NO\n",
    "q3 = \"\"\"Which reference was used to calculate the CCI? Return only the reference as Last Name Year. If it is not clear which reference was used to calculate the CCI, return NONE. \"\"\"\n",
    "\n",
    "# The following is only asked if question 2 = YES\n",
    "system_message1 = \"\"\"You are a literature review assistant. You will carefully review paragraphs and answer based on only what is present in the text. We are analyzing sentences from published literature to determine if it is clear which reference paper was used to calculate the Charlson Comorbidity Index (CCI) in this text. The following texts contain multiple references that might indicate how the Charlson Comorbidity Index (CCI) was calculated. We want to identify which reference or references were used to calculate the CCI in this paper. Sometimes these prompts will reference papers, but those were not used to calculate the CCI. Be strict.\n",
    "\n",
    "Here is an example: Which reference or references were used to calculate the CCI in this paragraph? Return only the reference as Last Name and Year. If it is not clear what was implemented, return None. \"In addition to DM, comorbidities defined by the Deyo's Charlson Comorbidity Index [Deyo 1992] were examined using a revised mapping algorithm cited by Quan et al. [Quan 2005].\" The correct response is Deyo 1992 and Quan 2005\n",
    "\n",
    "Here is an example:  Which reference or references were used to calculate the CCI in this paragraph? Return only the reference as Last Name and Year. If it is not clear what was implemented, return None. \"The Charlson Comorbidity Index is sometimes used to measure somatic comorbidity in schizophrenic patients. The index includes 19 severe chronic disorders that are assigned a weighted score according to severity. The index was originally constructed to quantify the impact of comorbidity on mortality in a hospital setting among breast cancer patients (Charlson 1987) and later was adapted to ICD-10 diagnoses (Sundararajan 2004).\" The correct response is Sundararajan 2004\n",
    "\n",
    "Here is an example: Which reference or references were used to calculate the CCI in this paragraph? Return only the reference as Last Name and Year. If it is not clear what was implemented, return None. \"We calculated the Charlson Comorbidity Index (Charlson 1987) using the (Quan 2005) implementation of ICD codes.\" The correct response is Quan 2005\n",
    "\n",
    "Here is an example: Which reference or references were used to calculate the CCI in this paragraph? Return only the reference as Last Name and Year. If it is not clear what was implemented, return None. \"For most applications, the CCI score is calculated by manual record review or using claims data, typically coded using the International Classification of Diseases, 9th Version (ICD-9).(Deyo 1992) (Romano 1993) (Quan 2005) The former approach is costly and the latter introduces biases due to coding errors, heterogeneous coding conventions, and the granularity of the coding system.16 In addition, previous research has suggested that manually extracting comorbidity information from medical records is superior to the use of ICD-9 codes17 and claims data are not available until after discharge time.\" The correct response is None \"\"\"\n",
    "\n",
    "q4 = \"\"\"Which reference or references were used to calculate or assess the CCI in this paragraph? Return only the reference as Last Name and Year. If it is not clear what was implemented, return None. \"\"\"\n",
    "\n",
    "# Create list for system messages and questions for easy reference\n",
    "system_message_list = [system_message0,system_message1]\n",
    "question_list = [q0,q1,q2,q3,q4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ddb0d2-44dd-4ad7-ab03-c6920b6b266d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_paper(paper_text: str,system_message_list: list[str],question_list: list[str],model_name: str,verbose=False) -> tuple[list,int]:\n",
    "    '''\n",
    "    Analyzes a research paper by querying a series of questions and collecting responses\n",
    "    to a LLM. Some questions can be skipped based on the response to initial questions. In\n",
    "    that case, NaN values are returned for follow-up questions. \n",
    "\n",
    "    PARAMETERS:\n",
    "    paper_text : str\n",
    "        The text of the research paper to be analyzed.\n",
    "    system_message_list : list\n",
    "        A list of system messages to be used in the queries.\n",
    "    question_list : list\n",
    "        A list of questions to be asked about the paper.\n",
    "    model_name : str\n",
    "        The name of the model to be used for the queries, from Azure AI Foundry\n",
    "    verbose : bool, optional\n",
    "        If True, prints the number of tokens used. Defaults to False.\n",
    "\n",
    "    RETURNS:\n",
    "    tuple : A tuple containing:\n",
    "        - result (list): A list of responses to the questions.\n",
    "        - count_tokens (int): The total number of tokens used in the queries.\n",
    "    '''\n",
    "    result = []\n",
    "    count_tokens = 0\n",
    "\n",
    "    # Format and ask the zeroth question\n",
    "    q0text = question_list[0] + paper_text\n",
    "    input = [SystemMessage(content=system_message_list[0]),\n",
    "             UserMessage(content=q0text)]\n",
    "    question, response, tokens_used = query(input,model_name)\n",
    "    count_tokens += tokens_used\n",
    "    result.append(response)\n",
    "    input.append(AssistantMessage(content=response))\n",
    "\n",
    "    # Format and ask the first question\n",
    "    q1text = question_list[1] + paper_text\n",
    "    input = [SystemMessage(content=system_message_list[0]),\n",
    "             UserMessage(content=q1text)]\n",
    "    question, response, tokens_used = query(input,model_name)\n",
    "    count_tokens += tokens_used\n",
    "    result.append(response)\n",
    "    input.append(AssistantMessage(content=response))\n",
    "\n",
    "    # If the response to either the 0th or 1st question\n",
    "    # is No, meaning there is not a reference,\n",
    "    # we don't need to ask the other questions\n",
    "    first_two_response_negative = any('no' in element.lower() for element in result)\n",
    "\n",
    "    if first_two_response_negative:\n",
    "        # Add NaN for other responses\n",
    "        result.append(np.nan) # Question 2\n",
    "        result.append(np.nan) # Question 3\n",
    "        result.append(np.nan) # Question 4\n",
    "    else:\n",
    "        # Format and ask the second question\n",
    "        q2text = question_list[2] + paper_text\n",
    "        input.append(UserMessage(content=q2text))\n",
    "        question, response, tokens_used = query(input,model_name)\n",
    "        count_tokens += tokens_used\n",
    "        result.append(response)\n",
    "        input.append(AssistantMessage(content=response))\n",
    "\n",
    "        # Q2 leads to a bifurcation\n",
    "        if 'no' in response.strip().lower(): \n",
    "            # Format and ask the third question\n",
    "            q3text = question_list[3] + paper_text\n",
    "            input.append(UserMessage(content=q3text))\n",
    "            question, response, tokens_used = query(input,model_name)\n",
    "            count_tokens += tokens_used\n",
    "            result.append(response)\n",
    "            input.append(AssistantMessage(content=response))\n",
    "\n",
    "            # Add NaN for question 4\n",
    "            result.append(np.nan) # question 4\n",
    "        else:\n",
    "            # Add NaN for responses 3\n",
    "            result.append(np.nan) #q3 response\n",
    "\n",
    "            # Add a new system message for the multi-reference papers\n",
    "            input.append(SystemMessage(content=system_message_list[1]))\n",
    "            \n",
    "            # Format and ask the fourth question\n",
    "            q4text = question_list[4] + paper_text\n",
    "            input.append(UserMessage(content=q4text))\n",
    "            question, response, tokens_used = query(input,model_name)\n",
    "            count_tokens += tokens_used\n",
    "            result.append(response)\n",
    "            input.append(AssistantMessage(content=response))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"This paper required {} tokens\".format(count_tokens))\n",
    "    return result, count_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df4746d8-b572-46a7-9cff-2cf061e640da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Full pipeline to pull paper from PMC and analyze it with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d99518-26d8-4078-a10c-18957f44a8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in file of PMC IDs, then save PMC ID values to a list\n",
    "pmc_id_df = pd.read_csv(\"YOUR_PATH_HERE\",header=0)\n",
    "pmc_id_list = pmc_id_df['PMCID'].tolist()\n",
    "\n",
    "# Create a DataFrame to store responses\n",
    "result_df = pd.DataFrame(columns=['pmc','paragraph','included_references','q0','q1','q2','q3','q4','tokens_used'])\n",
    "\n",
    "for x in range(len(pmc_id_list)):\n",
    "    try:\n",
    "        # Select 1 value from the PMCID list\n",
    "        query_id = pmc_id_list[x]\n",
    "\n",
    "        soup = fetchPMCxml(query_id,entrez_key)\n",
    "\n",
    "        # Find references\n",
    "        references = find_charlson_references(soup,search_criteria)\n",
    "\n",
    "        # Replace references in the body\n",
    "        soup_up_ref = replace_references_in_body(soup, references)\n",
    "        \n",
    "        # Parse into paragraphs\n",
    "        paragraph_dictionary = parse_pubmed_paragraph(soup_up_ref)\n",
    "\n",
    "        # Get the list of dictionaries with matching text\n",
    "        matching_paragraphs = filter_paragraphs(paragraph_dictionary, charlson_search_terms)\n",
    "\n",
    "        # Some texts will not include references. If there are no references, we do not\n",
    "        # need to analyze this article with the LLM\n",
    "        # But we do need to save this information so we know that is the case.\n",
    "        # One error check we can look for is when references are identified, but paragraphs not returned\n",
    "        # If there are references, we'll save those \n",
    "        if not matching_paragraphs:\n",
    "            if references.values():\n",
    "                result = [np.nan] + [list(references.values())] + [np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "            else:\n",
    "                result = [np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "            paragraph_df = pd.DataFrame([[query_id] + result],columns=['pmc','paragraph','included_references','q0','q1','q2','q3','q4','tokens_used'])\n",
    "            result_df = pd.concat([result_df, paragraph_df], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            # For each matching paragraph, analyze it with the LLM\n",
    "            for y in matching_paragraphs:\n",
    "                pmcid = y['pmc']\n",
    "                current_text = y['text']\n",
    "                current_references = list(references.values())\n",
    "                result, tokens_for_paragraph = analyze_paper(current_text,system_message_list,question_list,model_name,verbose=False)\n",
    "\n",
    "                # Create Dataframe for this paragraph\n",
    "                paragraph_df = pd.DataFrame([[pmcid] + [current_text] + [current_references] + result + [tokens_for_paragraph]],columns=['pmc','paragraph','included_references','q0','q1','q2','q3','q4','tokens_used'])\n",
    "                \n",
    "                # Append to the results dataframe\n",
    "                result_df = pd.concat([result_df, paragraph_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        result = ['ERROR','ERROR',np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "        paragraph_df = pd.DataFrame([[query_id] + result],columns=['pmc','paragraph','included_references','q0','q1','q2','q3','q4','tokens_used'])\n",
    "        # Append to the results dataframe\n",
    "        result_df = pd.concat([result_df, paragraph_df], ignore_index=True)\n",
    "\n",
    "    x += 1\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42cb22c5-2ae3-429a-b610-97328dde5668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Results DataFrame\n",
    "result_df.to_csv('YOUR_PATH_HERE')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1 CCI Literature Analysis with LLM",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}